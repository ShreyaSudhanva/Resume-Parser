{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import spacy\n",
    "from io import StringIO\n",
    "from importlib import reload\n",
    "import sys\n",
    "reload(sys)\n",
    "import pandas as pd\n",
    "#sys.setdefaultencoding('utf8')\n",
    "#from cStringIO import StringIO\n",
    "from pdfminer3.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer3.converter import TextConverter\n",
    "from pdfminer3.layout import LAParams\n",
    "from pdfminer3.pdfpage import PDFPage\n",
    "import os\n",
    "import sys, getopt\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "#import urllib2\n",
    "#from urllib2 import urlopen\n",
    "import urllib.request\n",
    "\n",
    "from pdfminer3.layout import LAParams, LTTextBox\n",
    "from pdfminer3.pdfpage import PDFPage\n",
    "from pdfminer3.pdfinterp import PDFResourceManager\n",
    "from pdfminer3.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer3.converter import PDFPageAggregator\n",
    "from pdfminer3.converter import TextConverter\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT TEXT\n",
    "\n",
    "resource_manager = PDFResourceManager()\n",
    "fake_file_handle = io.StringIO()\n",
    "converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
    "page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "\n",
    "# with open(\"Samanvitha_S.pdf\", 'rb') as fh:\n",
    "\n",
    "#     for page in PDFPage.get_pages(fh,\n",
    "#                                   caching=True,\n",
    "#                                   check_extractable=True):\n",
    "#         page_interpreter.process_page(page)\n",
    "\n",
    "#     text = fake_file_handle.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I am using\n",
    "\n",
    "d = 'resumes-trial'\n",
    "resumes = []\n",
    "def select_files_in_folder(d, ext):\n",
    "    for file in os.listdir(d):\n",
    "        if file.endswith('.%s' % ext):\n",
    "            yield os.path.join(d, file)\n",
    "\n",
    "\n",
    "for file in select_files_in_folder(d, 'pdf'):\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(resumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def convert_pdf_to_txt(path):\n",
    "#     rsrcmgr = PDFResourceManager()\n",
    "#     retstr = io.StringIO()\n",
    "#     codec = 'utf-8'\n",
    "#     laparams = LAParams()\n",
    "#     device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "#     fp = open('resumes-trial/Samanvitha_S.pdf', 'rb')\n",
    "#     interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "#     password = \"\"\n",
    "#     maxpages = 0\n",
    "#     caching = True\n",
    "#     pagenos = set()\n",
    "\n",
    "#     for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages,\n",
    "#                                   password=password,\n",
    "#                                   caching=caching,\n",
    "#                                   check_extractable=True):\n",
    "#         interpreter.process_page(page)\n",
    "\n",
    "\n",
    "\n",
    "#     fp.close()\n",
    "#     device.close()\n",
    "#     text = retstr.getvalue()\n",
    "#     retstr.close()\n",
    "#     return (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/samanvitha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/samanvitha/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/samanvitha/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/samanvitha/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "def extract_names(text):\n",
    "    person_names = []\n",
    "\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
    "                person_names.append(\n",
    "                    ' '.join(chunk_leave[0] for chunk_leave in chunk.leaves())\n",
    "                )\n",
    "\n",
    "    return person_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract Phone Numbers from string using regular expressions\n",
    "def extract_phone_numbers(string):\n",
    "    r = re.compile(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})')\n",
    "    phone_numbers = r.findall(string)\n",
    "    return [re.sub(r'\\D', '', number) for number in phone_numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email_addresses(string):\n",
    "    r = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "    return r.findall(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_information(string):\n",
    "#     string.replace (\" \", \"+\")\n",
    "#     query = string\n",
    "#     soup = BeautifulSoup(urllib.request.urlopen(\"https://en.wikipedia.org/wiki/\" + query), \"html.parser\")\n",
    "#     #creates soup and opens URL for Google. Begins search with site:wikipedia.com so only wikipedia\n",
    "#     #links show up. Uses html parser.\n",
    "#     for item in soup.find_all('div', attrs={'id' : \"mw-content-text\"}):\n",
    "#         print(item.find('p').get_text())\n",
    "#         print('\\n')\n",
    "# with open('techatt.csv', 'r') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     your_listatt = list(reader)\n",
    "with open('techskill.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    tech_skills = list(reader)\n",
    "# with open('nontechnicalskills.csv', 'r') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     your_list1 = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills():\n",
    "    skills = []\n",
    "    s = set(tech_skills[0])\n",
    "    skillindex = []\n",
    "    np_a1 = np.array(tech_skills)\n",
    "    for i in range(len(skills)):\n",
    "        item_index = np.where(np_a1==skills1[i])\n",
    "        skillindex.append(item_index[1][0])\n",
    "\n",
    "    nlen = len(skillindex)\n",
    "    for i in range(nlen):\n",
    "        print(skills1[i])\n",
    "        print(s2[0][skillindex[i]])\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = set(your_list[0])\n",
    "s1 = your_list\n",
    "s2 = your_listatt\n",
    "skillindex = []\n",
    "skills = []\n",
    "skillsatt = []\n",
    "print('\\n')\n",
    "print(extract_name(resume_string1))\n",
    "\n",
    "print('\\n')\n",
    "print('Phone Number is')\n",
    "y = extract_phone_numbers(resume_string)\n",
    "y1 = []\n",
    "for i in range(len(y)):\n",
    "    if(len(y[i])>9):\n",
    "        y1.append(y[i])\n",
    "print(y1)\n",
    "print('\\n')\n",
    "print('Email id is')\n",
    "print(extract_email_addresses(resume_string))\n",
    "for word in resume_string.split(\" \"):\n",
    "    if word in s:\n",
    "        skills.append(word)\n",
    "skills1 = list(set(skills))\n",
    "print('\\n')\n",
    "print(\"Following are his/her Technical Skills\")\n",
    "print('\\n')\n",
    "np_a1 = np.array(your_list)\n",
    "for i in range(len(skills1)):\n",
    "    item_index = np.where(np_a1==skills1[i])\n",
    "    skillindex.append(item_index[1][0])\n",
    "\n",
    "nlen = len(skillindex)\n",
    "for i in range(nlen):\n",
    "    print(skills1[i])\n",
    "    print(s2[0][skillindex[i]])\n",
    "    print('\\n')\n",
    "\n",
    "#Sets are used as it has a a constant time for lookup hence the overall the time for the total code will not exceed O(n)\n",
    "s1 = set(your_list1[0])\n",
    "nontechskills = []\n",
    "for word in resume_string.split(\" \"):\n",
    "    if word in s1:\n",
    "        nontechskills.append(word)\n",
    "nontechskills = set(nontechskills)\n",
    "print('\\n')\n",
    "\n",
    "print(\"Following are his/her Non Technical Skills\")\n",
    "list5 = list(nontechskills)\n",
    "print('\\n')\n",
    "for i in range(len(list5)):\n",
    "    print(list5[i])\n",
    "print('\\n \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####pypdf2 trial version\n",
    "\n",
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "for k in range(1,100):\n",
    "    # open the pdf file\n",
    "    object = PyPDF2.PdfFileReader(\"resumes-trial\".format(k))\n",
    "\n",
    "    # get number of pages\n",
    "    NumPages = object.getNumPages()\n",
    "\n",
    "    # define keyterms\n",
    "    String = \"New York State Real Property Law\"\n",
    "\n",
    "    # extract text and do the search\n",
    "    for i in range(0, NumPages):\n",
    "        PageObj = object.getPage(i)\n",
    "        print(\"this is page \" + str(i)) \n",
    "        Text = PageObj.extractText() \n",
    "        # print(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader\n",
    " \n",
    "# Creating a pdf file object.\n",
    "pdf = open(\"Samanvitha_S.pdf\", \"rb\")\n",
    "\n",
    "# Creating pdf reader object.\n",
    "pdf_reader = PyPDF2.PdfFileReader(pdf)\n",
    "print(\"Total number of Pages:\", pdf_reader.numPages)\n",
    " \n",
    "# Creating a page object.\n",
    "page = pdf_reader.getPage(0)\n",
    "print(page.extractText())\n",
    " \n",
    "# Closing the object.\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samanvitha S\n",
      "\n",
      "samanvitha.sateesha@gmail.com | (+91) 8904868690\n",
      "\n",
      "OBJECTIVE\n",
      "I am a Computer Science student pursuing\n",
      "my undergraduate degree and looking\n",
      "forward to utilise my knowledge to be an\n",
      "efﬁcient and valuable resource for the\n",
      "organisation while grooming my skills\n",
      "and expertise for personal growth and\n",
      "development.\n",
      "\n",
      "EDUCATION\n",
      "JSS SCIENCE AND\n",
      "TECHNOLOGY UNIVERSITY\n",
      "B.E. IN COMPUTER SCIENCE\n",
      "( 2018 - 2022 )\n",
      "Mysore, Karnataka\n",
      "Cum. GPA: 9.44/10.0 (4 Semesters)\n",
      "\n",
      "JNANODAYA PU COLLEGE\n",
      "2ND PU\n",
      "( 2016 - 2018 )\n",
      "Mysuru, Karnataka\n",
      "Percentage: 96%\n",
      "\n",
      "CHRIST PUBLIC SCHOOL\n",
      "CLASS X\n",
      "( 2009 - 2016 )\n",
      "Mysuru, Karnataka\n",
      "Cum. GPA: 10.0/10.0\n",
      "\n",
      "LINKS\n",
      "Github:// Samanvitha-Sateesha\n",
      "LinkedIn:// samanvitha-sateesha\n",
      "\n",
      "EXPERIENCE\n",
      "SITEX DIGITECH\n",
      "Management Intern | June 2020 - November 2020\n",
      "• Managed two projects of the company - Website Revamp and Education App Project.\n",
      "Used the knowledge of web development by contribute to the project with design ideas.\n",
      "• Was present in the education app project from the root till the fundamental features were\n",
      "successfully implemented. Formed a team of sound and eligible candidates and was involved\n",
      "in brainstorming sessions for the features of the app to weekly reviews and progress\n",
      "analysis.\n",
      "• Came up with the notion of launching a tic-tac-toe game on Instagram as a shout-out to\n",
      "the company.\n",
      "PROJECTS\n",
      "YELPCAMP | Node.js, MongoDB, Express.js, HTML, CSS, Bootstrap, JavaScript\n",
      "A dynamic application for campgrounds and features which allows users to create their own\n",
      "campgrounds, view and comment on others campground etc.\n",
      "\n",
      "SMART LIBRARY APPLICATION | HTML,CSS,JavaScript,Bootstrap, MySQL\n",
      "Smart Library Application has two modules, one each for the user and the admin. Admin can\n",
      "add, delete and update the information of the books. Users can place an order to borrow the\n",
      "books based on its availability.\n",
      "\n",
      "MY RESTFUL BLOG APP | Semantic UI\n",
      "A simple Blog App designed by following the conventions of RESTful routing that consists of\n",
      "all the 7 routes.\n",
      "\n",
      "ALGORITHMIC IMPLEMENTATION OF PDA | C++, PDA\n",
      "Designed a menu driven Push Down Automata to accept various types of Strings in C++\n",
      "CERTIFICATIONS\n",
      "THE WEB DEVELOPER BOOTCAMP\n",
      "• Udemy | April 2020 | [ Certificate]\n",
      "\n",
      "PYTHON DATA STRUCTURES\n",
      "• Coursera | September 2020 | [ Certificate]\n",
      "\n",
      "SKILLS\n",
      "PROGRAMMING\n",
      "• C • Java • HTML • CSS\n",
      "• Bootstrap • Python\n",
      "• Node.js •Express.js • MongoDB • MySQL\n",
      "\n",
      "PYTHON FOR EVERYBODY\n",
      "• Coursera | September 2020 | [ Certificate]\n",
      "ACHIEVEMENTS\n",
      "DSC WEBHOOKS | (2019 - 2020)\n",
      "• One of the winners of the front end web development competition.\n",
      "\n",
      "INSIDESHERPA | JPMORGAN CHASE CO. SOFTWARE ENGINEERING\n",
      "VIRTUAL EXPERIENCE | (May 2020 - July 2020)\n",
      "• Took part in a remote internship, completed the assigned modules and received a\n",
      "certiﬁcate.\n",
      "\n",
      "TOOLS\n",
      "• Git • LaTeX • GDB\n",
      "\n",
      "AFFILIATIONS\n",
      "INSTITUTION' S INNOVATION\n",
      "COUNCIL (IIC), JSS STU\n",
      "• Student Coordinator\n",
      "( Jan 2019 - Present )\n",
      "\n",
      "1\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "## This works and I am using it\n",
    "\n",
    "from pdfminer import high_level\n",
    "\n",
    "filename = \"Samanvitha_S.pdf\"\n",
    "pages = [0,1] # just the first page\n",
    "\n",
    "extracted_text = high_level.extract_text(filename, \"\", pages)\n",
    "print(extracted_text)\n",
    "#pri#nt(extract_names(extracted_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/samanvitha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/samanvitha/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/samanvitha/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/samanvitha/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "def extract_names(text):\n",
    "    person_names = []\n",
    "\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
    "                person_names.append(\n",
    "                    ' '.join(chunk_leave[0] for chunk_leave in chunk.leaves())\n",
    "                )\n",
    "\n",
    "    return person_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8553511856']\n",
      "['for', 'for']\n",
      "['9738959449']\n",
      "['for', 'for', 'for', 'for']\n",
      "['8904868690']\n",
      "['for', 'for', '(', ')', '(', ')', '(', ')', 'for', 'for', 'for', '(', ')']\n"
     ]
    }
   ],
   "source": [
    "d = 'resumes-trial'\n",
    "resumes_skills = []\n",
    "def select_files_in_folder(d, ext):\n",
    "    for file in os.listdir(d):\n",
    "        if file.endswith('.%s' % ext):\n",
    "            yield os.path.join(d, file)\n",
    "\n",
    "\n",
    "for file in select_files_in_folder(d, 'pdf'):\n",
    "    pages = [0,1] # just the first page\n",
    "    extracted_text = high_level.extract_text(file, \"\", pages)\n",
    "    #name = extract_names(extracted_text)\n",
    "    name = file[14:(len(file))-4]\n",
    "    number = extract_phone_numbers(extracted_text)\n",
    "    print(number)\n",
    "    resumes_skills = extract_skillsw(extracted_text)\n",
    "    print(resumes_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sets are used as it has a a constant time for lookup hence the overall the time for the total code will not exceed O(n)\n",
    "def extract_skillsw(resume_string):\n",
    "    skills = []\n",
    "    resume_tokens = []\n",
    "#     with open('techskill.csv', 'r') as f:\n",
    "#         reader = csv.reader(f)\n",
    "#         tech_skills = list(reader)\n",
    "    s = set(tech_skills)\n",
    "    for word in resume_string.split():\n",
    "        resume_tokens.append(word)\n",
    "        #print(resume_tokens)\n",
    "        #print(word)\n",
    "    for  word in resume_tokens:\n",
    "        if word in tech_skills:\n",
    "            skills.append(word)\n",
    "    return skills\n",
    "#     s1 = tech_skills\n",
    "#     skillindex = []\n",
    "#     skills = []\n",
    "#     for word in resume_string.split(\" \"):\n",
    "#         if word in s:\n",
    "#             skills.append(word)\n",
    "#     skills1 = list(set(skills))\n",
    "\n",
    "#     np_a1 = np.array(tech_skills)\n",
    "#     for i in range(len(skills1)):\n",
    "#         item_index = np.where(np_a1==skills1[i])\n",
    "#         skillindex.append(item_index[1][0])\n",
    "\n",
    "#     nlen = len(skillindex)\n",
    "#     for i in range(nlen):\n",
    "#         print(skills1[i])\n",
    "#         print(s2[0][skillindex[i]])\n",
    "#         print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347\n",
      "['ajenti', 'django-suit', 'django-xadmin', 'flask-admin', 'flower', 'grappelli', 'wooey', 'algorithms', 'pypattyrn', 'python-patterns', 'sortedcontainers', 'libraries', 'django-simple-captcha', 'django-simple-spam-blocker', 'django-compressor', 'django-pipeline', 'django-storages', 'fanstatic', 'fileconveyor', 'flask-assets', 'jinja-assets-compressor', 'webassets', 'audiolazy', 'audioread', 'beets', 'dejavu', 'django-elastic-transcoder', 'eyed3', 'id3reader', 'm3u8', 'mingus', 'pyaudioanalysis', 'pydub', 'pyechonest', 'talkbox', 'timeside', 'tinytag', 'authomatic', 'django-allauth', 'django-oauth-toolkit', 'flask-oauthlib', 'oauthlib', 'python-oauth2', 'python-social-auth', 'rauth', 'sanction', 'jose', 'pyjwt', 'python-jws', 'python-jwt', 'bitbake', 'buildout', 'platformio', 'pybuilder', 'scons', 'django-cms', 'djedi-cms', 'feincms', 'kotti', 'mezzanine', 'opps', 'plone', 'quokka', 'wagtail', 'widgy', 'libraries', 'beaker', 'diskcache', 'django-cache-machine', 'django-cacheops', 'django-viewlet', 'dogpile.cache', 'hermescache', 'johnny-cache', 'pylibmc', 'errbot', 'coala', 'code2flow', 'pycallgraph', 'flake8', 'pylama', 'pylint', 'mypy', 'asciimatics', 'cement', 'click', 'cliff', 'clint', 'colorama', 'docopt', 'gooey', 'python-fire', 'python-prompt-toolkit', 'aws-cli', 'bashplotlib', 'caniusepython3', 'cookiecutter', 'doitlive', 'howdoi', 'httpie', 'mycli', 'pathpicker', 'percol', 'pgcli', 'saws', 'thefuck', 'try', 'python-future', 'python-modernize', 'six', 'opencv', 'pyocr', 'pytesseract', 'simplecv', 'eventlet', 'gevent', 'multiprocessing', 'threading', 'tomorrow', 'uvloop', 'config', 'configobj', 'configparser', 'profig', 'python-decouple', 'cryptography', 'hashids', 'paramiko', 'passlib', 'pynacl', 'blaze', 'open', 'orange', 'pandas', 'cerberus', 'colander', 'jsonschema', 'schema', 'schematics', 'valideer', 'voluptuous', 'altair', 'bokeh', 'ggplot', 'matplotlib', 'pygal', 'pygraphviz', 'pyqtgraph', 'seaborn', 'vispy', 'pickledb', 'pipelinedb', 'tinydb', 'zodb', 'mysql', 'mysql-python', 'mysqlclient', 'oursql', 'pymysql', 'postgresql', 'psycopg2', 'queries', 'txpostgres', 'apsw', 'dataset', 'pymssql', 'nosql', 'cassandra-python-driver', 'happybase', 'plyvel', 'py2neo', 'pycassa', 'pymongo', 'redis-py', 'telephus', 'txredis', 'arrow', 'chronyk', 'dateutil', 'delorean', 'moment', 'pendulum', 'pytime', 'pytz', 'when.py', 'ipdb', 'pdb++', 'pudb', 'remote-pdb', 'wdb', 'line_profiler', 'memory_profiler', 'profiling', 'vprof', 'caffe', 'keras', 'mxnet', 'neupy', 'pytorch', 'tensorflow', 'theano', 'ansible', 'cloud-init', 'cuisine', 'docker', 'fabric', 'fabtools', 'honcho', 'openstack', 'pexpect', 'psutil', 'saltstack', 'supervisor', 'dh-virtualenv', 'nuitka', 'py2app', 'py2exe', 'pyinstaller', 'pynsist', 'sphinx', 'awesome-sphinxdoc', 'mkdocs', 'pdoc', 'pycco', 's3cmd', 's4cmd', 'you-get', 'youtube-dl', 'alipay', 'cartridge', 'django-oscar', 'django-shop', 'merchant', 'money', 'python-currencies', 'forex-python', 'shoop', 'emacs', 'elpy', 'sublime', 'anaconda', 'sublimejedi', 'vim', 'jedi-vim', 'python-mode', 'youcompleteme', 'ptvs', 'visual', 'python', 'magic', 'liclipse', 'pycharm', 'spyder', 'libraries', 'envelopes', 'flanker', 'imbox', 'inbox.py', 'lamson', 'marrow', 'modoboa', 'nylas', 'yagmail', 'pipenv', 'p', 'pyenv', 'venv', 'virtualenv', 'virtualenvwrapper', 'imghdr', 'mimetypes', 'path.py', 'pathlib', 'python-magic', 'unipath', 'watchdog', 'cffi', 'ctypes', 'pycuda', 'swig', 'deform', 'django-bootstrap3', 'django-crispy-forms', 'django-remote-forms', 'wtforms', 'cytoolz', 'fn.py', 'funcy', 'toolz', 'curses', 'enaml', 'flexx', 'kivy', 'pyglet', 'pygobject', 'pyqt', 'pyside', 'pywebview', 'tkinter', 'toga', 'urwid', 'wxpython', 'cocos2d', 'panda3d', 'pygame', 'pyogre', 'pyopengl', 'pysdl2', 'renpy', 'django-countries', 'geodjango', 'geoip', 'geojson', 'geopy', 'pygeoip', 'beautifulsoup', 'bleach', 'cssutils', 'html5lib', 'lxml', 'markupsafe', 'pyquery', 'untangle', 'weasyprint', 'xmldataset', 'xmltodict', 'grequests', 'httplib2', 'requests', 'treq', 'urllib3', 'ino', 'keyboard', 'mouse', 'pingo', 'pyro', 'pyuserinput', 'scapy', 'wifi', 'hmap', 'imgseek', 'nude.py', 'pagan', 'pillow', 'pybarcode', 'pygram', 'python-qrcode', 'quads', 'scikit-image', 'thumbor', 'wand', 'clpython', 'cpython', 'cython', 'grumpy', 'ironpython', 'jython', 'micropython', 'numba', 'peachpy', 'pyjion', 'pypy', 'pysec', 'pyston', 'stackless', 'interactive', 'bpython', 'jupyter', 'ptpython', 'babel', 'pyicu', 'apscheduler', 'django-schedule', 'doit', 'gunnery', 'joblib', 'plan', 'schedule', 'spiff', 'taskflow', 'eliot', 'logbook', 'logging', 'sentry', 'metrics', 'nupic', 'scikit-learn', 'spark', 'vowpal_porpoise', 'xgboost', 'pyspark', 'luigi', 'mrjob', 'streamparse', 'dask', 'python', '(', 'x', 'y', ')', 'pythonlibs', 'pythonnet', 'pywin32', 'winpython', 'gensim', 'jieba', 'langid.py', 'nltk', 'pattern', 'polyglot', 'snownlp', 'spacy', 'textblob', 'mininet', 'pox', 'pyretic', 'sdx', 'asyncio', 'diesel', 'pulsar', 'pyzmq', 'twisted', 'txzmq', 'napalm', 'django-activity-stream', 'stream-framework', 'django', 'sqlalchemy', 'awesome-sqlalchemy', 'orator', 'peewee', 'ponyorm', 'pydal', 'python-sql', 'pip', 'python', 'conda', 'curdling', 'pip-tools', 'wheel', 'warehouse', 'warehouse', 'bandersnatch', 'devpi', 'localshop', 'carteblanche', 'django-guardian', 'django-rules', 'delegator.py', 'subprocesses', 'for', 'sarge', 'sh', 'celery', 'huey', 'mrq', 'rq', 'simpleq', 'annoy', 'fastfm', 'implicit', 'libffm', 'lightfm', 'surprise', 'tensorrec', 'django-rest-framework', 'django-tastypie', 'flask', 'eve', 'flask-api-utils', 'flask-api', 'flask-restful', 'flask-restless', 'pyramid', 'cornice', 'framework', 'falcon', 'hug', 'restless', 'ripozo', 'sandman', 'apistar', 'simplejsonrpcserver', 'simplexmlrpcserver', 'zerorpc', 'astropy', 'bcbio-nextgen', 'bccb', 'biopython', 'cclib', 'networkx', 'nipy', 'numpy', 'open', 'obspy', 'pydy', 'pymc', 'rdkit', 'scipy', 'statsmodels', 'sympy', 'zipline', 'simpy', 'django-haystack', 'elasticsearch-dsl-py', 'elasticsearch-py', 'esengine', 'pysolr', 'solrpy', 'whoosh', 'marshmallow', 'apex', 'python-lambda', 'zappa', 'tablib', 'marmir', 'openpyxl', 'pyexcel', 'python-docx', 'relatorio', 'unoconv', 'xlsxwriter', 'xlwings', 'xlwt', '/', 'xlrd', 'pdf', 'pdfminer', 'pypdf2', 'reportlab', 'markdown', 'mistune', 'python-markdown', 'yaml', 'pyyaml', 'csvkit', 'unp', 'cactus', 'hyde', 'lektor', 'nikola', 'pelican', 'tinkerer', 'django-taggit', 'genshi', 'jinja2', 'mako', 'hypothesis', 'mamba', 'nose', 'nose2', 'pytest', 'robot', 'unittest', 'green', 'tox', 'locust', 'pyautogui', 'selenium', 'sixpack', 'splinter', 'doublex', 'freezegun', 'httmock', 'httpretty', 'mock', 'responses', 'vcr.py', 'factory_boy', 'mixer', 'model_mommy', 'mimesis', 'fake2db', 'faker', 'radar', 'chardet', 'difflib', 'ftfy', 'fuzzywuzzy', 'levenshtein', 'pangu.py', 'pyfiglet', 'pypinyin', 'shortuuid', 'unidecode', 'uniout', 'xpinyin', 'slugify', 'awesome-slugify', 'python-slugify', 'unicode-slugify', 'parser', 'phonenumbers', 'ply', 'pygments', 'pyparsing', 'python-nameparser', 'python-user-agents', 'sqlparse', 'apache-libcloud', 'boto3', 'django-wordpress', 'facebook-sdk', 'facepy', 'gmail', 'google-api-python-client', 'gspread', 'twython', 'furl', 'purl', 'pyshorteners', 'short_url', 'webargs', 'moviepy', 'scikit-video', 'wsgi-compatible', 'bjoern', 'fapws3', 'gunicorn', 'meinheld', 'netius', 'paste', 'rocket', 'uwsgi', 'waitress', 'werkzeug', 'haul', 'html2text', 'lassie', 'micawber', 'newspaper', 'opengraph', 'python-goose', 'python-readability', 'sanitize', 'sumy', 'textract', 'cola', 'demiurge', 'feedparser', 'grab', 'mechanicalsoup', 'portia', 'pyspider', 'robobrowser', 'scrapy', 'bottle', 'cherrypy', 'django', 'awesome-django', 'flask', 'awesome-flask', 'pyramid', 'awesome-pyramid', 'sanic', 'tornado', 'turbogears', 'web2py', 'github', 'autobahnpython', 'crossbar', 'django-socketio', 'websocket-for-python', 'javascript', 'php', 'c', '#', 'c++', 'ruby', 'css', 'c', 'objective-c', 'shell', 'scala', 'swift', 'matlab', 'clojure', 'octave']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "reader = open(\"techskill.csv\", \"r\")\n",
    "tokens = []\n",
    "for line in reader:\n",
    "    tokens.extend(word_tokenize(line))\n",
    "# print(tokens)\n",
    "print(len(tokens))\n",
    "tech_skills = [word for word in tokens if word is not ',']\n",
    "#print(tech_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
